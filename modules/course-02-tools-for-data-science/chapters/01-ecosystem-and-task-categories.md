# Course 2, Module 1: Ecosystem and Task Categories

This chapter clarified the categories of work in data science and the supporting platforms that make the work repeatable, secure, and collaborative.

## Data Science task categories
- Data Management: storage, governance, and retrieval of data assets
- Data Integration & Transformation: ETL/ELT pipelines, orchestration, automation
- Data Visualization: charts, dashboards, and narrative communication
- Modeling: model building, deployment, monitoring, and assessment

## Supporting capabilities
- Code Asset Management: versioning, reviews, issues (e.g., Git/GitHub)
- Data Asset Management: catalogs, access control, backups, lineage
- Development Environments: notebooks and IDEs for building and testing
- Execution Environments: compute for batch, streaming, and interactive workloads

## Tooling landscape
- Open source and commercial tools across desktop, server, and cloud
- Web-based UIs and APIs enable collaboration and portability

## Representative tools by category
- Data Management: MySQL, PostgreSQL (open source); IBM Db2, SQL Server (commercial/cloud)
- NoSQL: MongoDB, Apache Cassandra
- Big Data: Apache Hadoop, Apache Spark
- Integration/Streaming: Apache Airflow, Apache Kafka
- Visualization: Cognos Analytics, Tableau, Power BI
- Dev Environments: Jupyter Notebooks, RStudio

## Takeaways
- The ecosystem is broad; choose tools that fit use case, scale, and team workflows.
- Separate concerns: manage code, data, development, and execution environments explicitly.
